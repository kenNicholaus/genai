{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Uyp7dZgC2csY",
   "metadata": {
    "id": "Uyp7dZgC2csY"
   },
   "outputs": [],
   "source": [
    "!pip install \"shapely<2.0.0\"\n",
    "! pip install google-cloud-aiplatform langchain chromadb pydantic typing-inspect typing_extensions pandas datasets google-api-python-client pypdf faiss-cpu transformers config --upgrade --user\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "KIDX4jqCiZOq",
   "metadata": {
    "id": "KIDX4jqCiZOq"
   },
   "outputs": [],
   "source": [
    "# Restart kernel after installs so that your environment can access the new packages\n",
    "import IPython\n",
    "\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "KuRs5JKJo1s3",
   "metadata": {
    "id": "KuRs5JKJo1s3"
   },
   "source": [
    "# LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fhextIUkrf2n",
   "metadata": {
    "id": "fhextIUkrf2n"
   },
   "outputs": [],
   "source": [
    "import vertexai\n",
    "import langchain\n",
    "from langchain.llms import VertexAI\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "PROJECT_ID = \"vertext-ai-dar\"  # @param {type:\"string\"}\n",
    "vertexai.init(project=PROJECT_ID, location=\"us-central1\")\n",
    "\n",
    "print(f\"LangChain version: {langchain.__version__}\")\n",
    "print(f\"Vertex AI SDK version: {aiplatform.__version__}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DTt24fTh27sH",
   "metadata": {
    "id": "DTt24fTh27sH"
   },
   "outputs": [],
   "source": [
    "llm = VertexAI(\n",
    "    model_name=\"text-bison@001\",\n",
    "    max_output_tokens=256,\n",
    "    temperature=0.8,\n",
    "    top_p=0.8,\n",
    "    top_k=40,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "llm(\"Tell me a joke\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ushK-OAj3DsJ",
   "metadata": {
    "id": "ushK-OAj3DsJ"
   },
   "outputs": [],
   "source": [
    "prompt_1 = \"List the key dates in the life of Steve Jobs\"\n",
    "\n",
    "prompt_2 = \"\"\"\n",
    "    Classify the following question as one of the following: Computers, Audio-Video, or Appliances.\n",
    "\n",
    "    Question: Show me your TVs.\n",
    "    Answer: Audio-Video\n",
    "\n",
    "    Question: Do you have any good deals on PCs?\n",
    "    Answer:\n",
    "\"\"\"\n",
    "\n",
    "prompt_3 = \"\"\"\n",
    "    Context: You answer questions about dogs.\n",
    "\n",
    "    If someone asks a questions about cats just return \"Woof, I don't know\"\n",
    "\n",
    "    Q: What is a good breed of dog for kids?\n",
    "    A: Golden Retrieves are nice. There are lots of of good dogs too.\n",
    "\n",
    "    Q: What is the best treat for cats?\n",
    "    A: Woof, I don't know\n",
    "\n",
    "    Q: How can I get my cat to stop attacking my dog?\n",
    "    A:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "llm(prompt_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XtwB4EUwLKDa",
   "metadata": {
    "id": "XtwB4EUwLKDa"
   },
   "source": [
    "# Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nl3KOi04Mer0",
   "metadata": {
    "id": "nl3KOi04Mer0"
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Context: You write in the style of {style}.\n",
    "    Write me a {output} about {thing}.\n",
    "    \"\"\"\n",
    ")\n",
    "prompt_template.format(style=\"a pirate\", output=\"poem\", thing=\"COBOL Programming\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kIN96pKrOTIr",
   "metadata": {
    "id": "kIN96pKrOTIr"
   },
   "outputs": [],
   "source": [
    "llm(prompt_template.format(style=\"a pirate\", output=\"poem\", thing=\"COBOL Programming\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_gFDmb2PSpZs",
   "metadata": {
    "id": "_gFDmb2PSpZs"
   },
   "source": [
    "# Custom Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dBowPOo-So40",
   "metadata": {
    "id": "dBowPOo-So40"
   },
   "outputs": [],
   "source": [
    "def rb(arg):\n",
    "    r = 0\n",
    "    b = arg.bit_length()\n",
    "\n",
    "    for i in range(b):\n",
    "        r <<= 1\n",
    "        r |= (arg & 1)\n",
    "        r >>= 1\n",
    "\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "EnlFOiPFTAnj",
   "metadata": {
    "id": "EnlFOiPFTAnj"
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import StringPromptTemplate\n",
    "from pydantic import BaseModel, validator\n",
    "import inspect\n",
    "\n",
    "PROMPT = \"\"\"\n",
    "Given the function name and source code, generate an English language explanation of the function.\n",
    "Function Name: {function_name}\n",
    "Source Code:\n",
    "{source_code}\n",
    "Explanation:\n",
    "\"\"\"\n",
    "\n",
    "class FunctionExplainerPromptTemplate(StringPromptTemplate, BaseModel):\n",
    "    \"\"\"A custom prompt template that takes in the function name as input,\n",
    "    and formats the prompt template to provide the source code of the function.\"\"\"\n",
    "\n",
    "    @validator(\"input_variables\")\n",
    "    def validate_input_variables(cls, v):\n",
    "        \"\"\"Validate that the input variables are correct.\"\"\"\n",
    "        if len(v) != 1 or \"function_name\" not in v:\n",
    "            raise ValueError(\"function_name must be the only input_variable.\")\n",
    "        return v\n",
    "\n",
    "    def format(self, **kwargs) -> str:\n",
    "        # Get the source code of the function\n",
    "        source_code = inspect.getsource(kwargs[\"function_name\"])\n",
    "\n",
    "        # Generate the prompt to be sent to the language model\n",
    "        prompt = PROMPT.format(\n",
    "            function_name=kwargs[\"function_name\"].__name__, source_code=source_code\n",
    "        )\n",
    "        return prompt\n",
    "\n",
    "    def _prompt_type(self):\n",
    "        return \"function-explainer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "CZS_VkNvTQ-g",
   "metadata": {
    "id": "CZS_VkNvTQ-g"
   },
   "outputs": [],
   "source": [
    "fn_explainer = FunctionExplainerPromptTemplate(input_variables=[\"function_name\"])\n",
    "\n",
    "# Generate a prompt for the function \"get_source_code\"\n",
    "prompt = fn_explainer.format(function_name=rb)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mDs1nBqFThgi",
   "metadata": {
    "id": "mDs1nBqFThgi"
   },
   "outputs": [],
   "source": [
    "llm(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "VODWLAeUW0ci",
   "metadata": {
    "id": "VODWLAeUW0ci"
   },
   "source": [
    "# Prompt Template Pipelining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9SRU2wiMW5va",
   "metadata": {
    "id": "9SRU2wiMW5va"
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "prompt = (\n",
    "    PromptTemplate.from_template(\"\"\"Tell me a joke about {topic},\n",
    "    make it funny and in {language}\"\"\")\n",
    ")\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "chain.invoke({\"topic\": \"COBOL\", \"language\": \"English\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZBQ9qC_GZEWC",
   "metadata": {
    "id": "ZBQ9qC_GZEWC"
   },
   "source": [
    "# Output Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cHj9C22jZHsM",
   "metadata": {
    "id": "cHj9C22jZHsM"
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.pydantic_v1 import BaseModel, Field, validator\n",
    "\n",
    "# Define your desired data structure.\n",
    "class Joke(BaseModel):\n",
    "    setup: str = Field(description=\"question to set up a joke\")\n",
    "    punchline: str = Field(description=\"answer to resolve the joke\")\n",
    "\n",
    "    # You can add custom validation logic easily with Pydantic.\n",
    "    @validator(\"setup\")\n",
    "    def question_ends_with_question_mark(cls, field):\n",
    "        if field[-1] != \"?\":\n",
    "            raise ValueError(\"Badly formed question!\")\n",
    "        return field\n",
    "\n",
    "# Set up a parser + inject instructions into the prompt template.\n",
    "parser = PydanticOutputParser(pydantic_object=Joke)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "# And a query intended to prompt a language model to populate the data structure.\n",
    "prompt_and_model = prompt | llm\n",
    "output = prompt_and_model.invoke({\"query\": \"Tell me a joke about Python programming.\"})\n",
    "\n",
    "parser.invoke(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xzejLH-sbGyb",
   "metadata": {
    "id": "xzejLH-sbGyb"
   },
   "outputs": [],
   "source": [
    "# Here's another example, but with a compound typed field.\n",
    "class Actor(BaseModel):\n",
    "    name: str = Field(description=\"name of an actor\")\n",
    "    film_names: List[str] = Field(description=\"list of names of films they starred in\")\n",
    "\n",
    "\n",
    "actor_query = \"Generate the filmography for a random actor.\"\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=Actor)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "_input = prompt.format_prompt(query=actor_query)\n",
    "\n",
    "output = llm(_input.to_string())\n",
    "\n",
    "parser.parse(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Dot-w_1gG4Ns",
   "metadata": {
    "id": "Dot-w_1gG4Ns"
   },
   "source": [
    "# List Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lTR-eDcscIGB",
   "metadata": {
    "id": "lTR-eDcscIGB"
   },
   "outputs": [],
   "source": [
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "output_parser = CommaSeparatedListOutputParser()\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"List five {subject}, Only list the items with no formatting.\n",
    "    {format_instructions}\"\"\",\n",
    "    input_variables=[\"subject\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions}\n",
    ")\n",
    "\n",
    "_input = prompt.format(subject=\"ice cream flavors\")\n",
    "output = llm(_input)\n",
    "\n",
    "output_parser.parse(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ukXYqwK5cIdY",
   "metadata": {
    "id": "ukXYqwK5cIdY"
   },
   "outputs": [],
   "source": [
    "# Here's another example, but with a compound typed field.\n",
    "class Athlete(BaseModel):\n",
    "    name: str = Field(description=\"name of an athlete\")\n",
    "    teams: List[str] = Field(description=\"list of teams they played for\")\n",
    "\n",
    "\n",
    "athlete_query = \"Name a random NFL Quarterback from the 1900s.\"\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=Athlete)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "_input = prompt.format_prompt(query=athlete_query)\n",
    "\n",
    "output = llm(_input.to_string())\n",
    "\n",
    "parser.parse(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ciLIR8e--xdf",
   "metadata": {
    "id": "ciLIR8e--xdf"
   },
   "source": [
    "# LCEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09AIem63-wbl",
   "metadata": {
    "id": "09AIem63-wbl"
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import StrOutputParser\n",
    "\n",
    "prompt = (\n",
    "    PromptTemplate.from_template(\"Write me a poem about {topic}\"\n",
    "    + \", make it rhyme\"\n",
    "    + \"\\n\\nand in {language}\")\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# LCEL Syntax\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "for chunk in chain.stream({\"topic\": \"COBOL\", \"language\": \"English\"}):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "X-_3nkx6Mf1v",
   "metadata": {
    "id": "X-_3nkx6Mf1v"
   },
   "source": [
    "# Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "S_c5RdAR44GQ",
   "metadata": {
    "id": "S_c5RdAR44GQ"
   },
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatVertexAI\n",
    "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "chat = ChatVertexAI(model_name=\"chat-bison@001\",\n",
    "    max_output_tokens=256,\n",
    "    temperature=0.1,\n",
    "    top_p=0.8,\n",
    "    top_k=40,\n",
    "    verbose=True,)\n",
    "\n",
    "chat([HumanMessage(content=\"What's a good recipe for a Halloween Party?\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DpHLqg0fHLhM",
   "metadata": {
    "id": "DpHLqg0fHLhM"
   },
   "outputs": [],
   "source": [
    "response = chat(\n",
    "    [\n",
    "        SystemMessage(content=\"You are a bot who knows about cooking\"),\n",
    "        HumanMessage(content=\"What's a good desert for Thanksgiving\"),\n",
    "        AIMessage(content=\"Pumpkin pie is always a winner.\"),\n",
    "        HumanMessage(content=\"Great, what is the recipe?\")\n",
    "    ]\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "H4VZiEAKO9EW",
   "metadata": {
    "id": "H4VZiEAKO9EW"
   },
   "source": [
    "# ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "VRuE-jHEO7CY",
   "metadata": {
    "id": "VRuE-jHEO7CY"
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful {job}. Your name is {name}.\"),\n",
    "        (\"human\", \"Hello, how are you doing?\"),\n",
    "        (\"ai\", \"I'm doing well, thanks!\"),\n",
    "        (\"human\", \"{user_input}\"),\n",
    "    ]\n",
    ")\n",
    "messages = chat_template.format_messages(\n",
    "                      job=\"Chef\",\n",
    "                      name=\"Julia\",\n",
    "                      user_input=\"What is your name and what do you do?\")\n",
    "\n",
    "chat(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "VX5FX11EFZWp",
   "metadata": {
    "id": "VX5FX11EFZWp"
   },
   "source": [
    "# Chat with Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "G6AMfE10-NUs",
   "metadata": {
    "id": "G6AMfE10-NUs"
   },
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "conversation = ConversationChain(llm=chat, memory=memory, verbose=False)\n",
    "\n",
    "input = \"\"\"\n",
    "    System: You are a Chef named Julia.\n",
    "    Human: What is a good recipe for dinner that includes bananas?\n",
    "\"\"\"\n",
    "\n",
    "conversation.predict(input = input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "P-ZouKyPFELj",
   "metadata": {
    "id": "P-ZouKyPFELj"
   },
   "outputs": [],
   "source": [
    "conversation.predict(input = \"How long would that take to prepare?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Id4DetKhFju6",
   "metadata": {
    "id": "Id4DetKhFju6"
   },
   "outputs": [],
   "source": [
    "conversation.predict(input=\"Should those be served warm or chilled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wLpT3ISsLceq",
   "metadata": {
    "id": "wLpT3ISsLceq"
   },
   "outputs": [],
   "source": [
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "EDy2Kptjsx6_",
   "metadata": {
    "id": "EDy2Kptjsx6_"
   },
   "outputs": [],
   "source": [
    "from langchain.embeddings import VertexAIEmbeddings\n",
    "\n",
    "# Utility functions for Embeddings API with rate limiting\n",
    "def rate_limit(max_per_minute):\n",
    "    period = 60 / max_per_minute\n",
    "    print(\"Waiting\")\n",
    "    while True:\n",
    "        before = time.time()\n",
    "        yield\n",
    "        after = time.time()\n",
    "        elapsed = after - before\n",
    "        sleep_time = max(0, period - elapsed)\n",
    "        if sleep_time > 0:\n",
    "            print(\".\", end=\"\")\n",
    "            time.sleep(sleep_time)\n",
    "\n",
    "\n",
    "class CustomVertexAIEmbeddings(VertexAIEmbeddings, BaseModel):\n",
    "    requests_per_minute: int\n",
    "    num_instances_per_batch: int\n",
    "\n",
    "    # Overriding embed_documents method\n",
    "    def embed_documents(self, texts: List[str]):\n",
    "        limiter = rate_limit(self.requests_per_minute)\n",
    "        results = []\n",
    "        docs = list(texts)\n",
    "\n",
    "        while docs:\n",
    "            # Working in batches because the API accepts maximum 5\n",
    "            # documents per request to get embeddings\n",
    "            head, docs = (\n",
    "                docs[: self.num_instances_per_batch],\n",
    "                docs[self.num_instances_per_batch :],\n",
    "            )\n",
    "            chunk = self.client.get_embeddings(head)\n",
    "            results.extend(chunk)\n",
    "            next(limiter)\n",
    "\n",
    "        return [r.values for r in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DY8wWTqpsvT0",
   "metadata": {
    "id": "DY8wWTqpsvT0"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Utility functions for Embeddings API with rate limiting\n",
    "def rate_limit(max_per_minute):\n",
    "    period = 60 / max_per_minute\n",
    "    print(\"Waiting\")\n",
    "    while True:\n",
    "        before = time.time()\n",
    "        yield\n",
    "        after = time.time()\n",
    "        elapsed = after - before\n",
    "        sleep_time = max(0, period - elapsed)\n",
    "        if sleep_time > 0:\n",
    "            print(\".\", end=\"\")\n",
    "            time.sleep(sleep_time)\n",
    "\n",
    "class CustomVertexAIEmbeddings(VertexAIEmbeddings, BaseModel):\n",
    "    requests_per_minute: int\n",
    "    num_instances_per_batch: int\n",
    "\n",
    "    # Overriding embed_documents method\n",
    "    def embed_documents(self, texts: List[str]):\n",
    "        limiter = rate_limit(self.requests_per_minute)\n",
    "        results = []\n",
    "        docs = list(texts)\n",
    "\n",
    "        while docs:\n",
    "            # Working in batches because the API accepts maximum 5\n",
    "            # documents per request to get embeddings\n",
    "            head, docs = (\n",
    "                docs[: self.num_instances_per_batch],\n",
    "                docs[self.num_instances_per_batch :],\n",
    "            )\n",
    "            chunk = self.client.get_embeddings(head)\n",
    "            results.extend(chunk)\n",
    "            next(limiter)\n",
    "\n",
    "        return [r.values for r in results]\n",
    "\n",
    "# Embedding\n",
    "EMBEDDING_QPM = 100\n",
    "EMBEDDING_NUM_BATCH = 5\n",
    "embeddings = CustomVertexAIEmbeddings(\n",
    "    requests_per_minute=EMBEDDING_QPM,\n",
    "    num_instances_per_batch=EMBEDDING_NUM_BATCH,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
