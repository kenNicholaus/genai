{
  "cells": [
    {
      "cell_type": "code",
      "id": "Y0r19Litu1yTVwvcGAGgt1IF",
      "metadata": {
        "tags": [],
        "id": "Y0r19Litu1yTVwvcGAGgt1IF"
      },
      "source": [
        "!pip install --upgrade --user google-cloud-aiplatform>=1.29.0 google-cloud-storage langchain chromadb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Restart kernel after installs so that your environment can access the new packages\n",
        "import IPython\n",
        "\n",
        "app = IPython.Application.instance()\n",
        "app.kernel.do_shutdown(True)"
      ],
      "metadata": {
        "id": "HMBswhDCuk2E"
      },
      "id": "HMBswhDCuk2E",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set Variables"
      ],
      "metadata": {
        "id": "sVvl1hSGcvzQ"
      },
      "id": "sVvl1hSGcvzQ"
    },
    {
      "cell_type": "code",
      "source": [
        "# get project ID\n",
        "PROJECT_ID = ! gcloud config get-value project\n",
        "PROJECT_ID = PROJECT_ID[0]\n",
        "LOCATION = \"us-central1\" # @param {type:\"string\"}\n",
        "DOCUMENT_URL = \"https://www.gutenberg.org/cache/epub/55/pg55.txt\" # @param {type:\"string\"}\n",
        "\n",
        "# define project information manually if the above code didn't work\n",
        "if PROJECT_ID == \"(unset)\":\n",
        "  PROJECT_ID = \"[your-project-id]\" # @param {type:\"string\"}\n",
        "\n",
        "print(PROJECT_ID)"
      ],
      "metadata": {
        "id": "it17qlHBurGV"
      },
      "id": "it17qlHBurGV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "# getpass will prompt for an API Key\n",
        "# The API Key is needed for Chroma DB\n",
        "API_KEY = getpass.getpass(\"Provide your Google API Key\")"
      ],
      "metadata": {
        "id": "fidCO61-pWA_"
      },
      "id": "fidCO61-pWA_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initial Vertex AI"
      ],
      "metadata": {
        "id": "MfUnQ4QIc2aT"
      },
      "id": "MfUnQ4QIc2aT"
    },
    {
      "cell_type": "code",
      "source": [
        "# init the aiplatform package\n",
        "from google.cloud import aiplatform\n",
        "aiplatform.init(project=PROJECT_ID, location=LOCATION)"
      ],
      "metadata": {
        "id": "47xjFhdUhYYY"
      },
      "execution_count": null,
      "outputs": [],
      "id": "47xjFhdUhYYY"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Just test the Embeddings model"
      ],
      "metadata": {
        "id": "1jujZHnvc60j"
      },
      "id": "1jujZHnvc60j"
    },
    {
      "cell_type": "code",
      "source": [
        "from vertexai.language_models import TextEmbeddingModel\n",
        "\n",
        "def text_embedding(text_to_embed) -> list:\n",
        "    \"\"\"Text embedding with a Large Language Model.\"\"\"\n",
        "    model = TextEmbeddingModel.from_pretrained(\"textembedding-gecko@002\")\n",
        "    embeddings = model.get_embeddings([text_to_embed])\n",
        "    for embedding in embeddings:\n",
        "        vector = embedding.values\n",
        "        print(f\"Length of Embedding Vector: {len(vector)}\")\n",
        "    return vector"
      ],
      "metadata": {
        "id": "vtO0EPFDvz81"
      },
      "id": "vtO0EPFDvz81",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emb1 = text_embedding(\"Hello World\")\n",
        "print(emb1)"
      ],
      "metadata": {
        "id": "aTkaz6IixJbY"
      },
      "id": "aTkaz6IixJbY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from vertexai.language_models import TextEmbeddingInput\n",
        "\n",
        "emd_with_task_type  = TextEmbeddingInput(\n",
        "    text=\"Hello World\",\n",
        "    task_type=\"RETRIEVAL_QUERY\"\n",
        ")\n",
        "\n",
        "emb2 = text_embedding(emd_with_task_type)\n",
        "print(emb2)\n",
        "\n",
        "\n",
        "emd_with_task_type  = TextEmbeddingInput(\n",
        "    text=\"Hello World\",\n",
        "    task_type=\"RETRIEVAL_DOCUMENT\"\n",
        ")\n",
        "\n",
        "emb3 = text_embedding(emd_with_task_type)\n",
        "print(emb3)"
      ],
      "metadata": {
        "id": "923UgZTfHbnR"
      },
      "id": "923UgZTfHbnR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(emb1 == emb2)\n",
        "print(emb2 == emb3)"
      ],
      "metadata": {
        "id": "tReOh4EbIXwI"
      },
      "id": "tReOh4EbIXwI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the document that you want to analyze with the LLM"
      ],
      "metadata": {
        "id": "5TlkgCazdCe-"
      },
      "id": "5TlkgCazdCe-"
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "\n",
        "loader = WebBaseLoader(DOCUMENT_URL)\n",
        "\n",
        "data = loader.load()\n",
        "document = data[0].page_content\n",
        "\n",
        "# Print first 50 characters\n",
        "print(document[:50])"
      ],
      "metadata": {
        "id": "vwwGSkKPxqGw"
      },
      "id": "vwwGSkKPxqGw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The document needs to be split into chucks.\n",
        "\n",
        "Chunk size and chunk overlap are the interesting variables. Each chunk will be represented by an embedding. Larger chunks will mean there is more data for the LLM to analyze. Smaller chunks will mean less data will be passed to the LLM at inference time.\n",
        "\n",
        "Smaller chunks also mean more embeddings."
      ],
      "metadata": {
        "id": "Lp5rjtHedVzM"
      },
      "id": "Lp5rjtHedVzM"
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "CHUNK_SIZE = 10000\n",
        "CHUNK_OVERLAP = 20\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=CHUNK_SIZE,\n",
        "    chunk_overlap=CHUNK_OVERLAP,\n",
        "    length_function=len,\n",
        "    is_separator_regex=False,\n",
        ")\n",
        "\n",
        "chunks = text_splitter.create_documents([document])\n",
        "\n",
        "# Convert chunks into list of strings\n",
        "chunks = [chunk.page_content for chunk in chunks]\n",
        "\n",
        "print(len(chunks))\n",
        "print(chunks[10][:50])"
      ],
      "metadata": {
        "id": "qHJTncIV1OvC"
      },
      "id": "qHJTncIV1OvC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate the embeddings in batches\n",
        "\n",
        "Below are helper functions that are used to rate limit and batch the geberation of the embeddings."
      ],
      "metadata": {
        "id": "Okd3WPTndN3B"
      },
      "id": "Okd3WPTndN3B"
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Generator, List, Optional, Tuple\n",
        "import functools\n",
        "import time\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import math\n",
        "\n",
        "\n",
        "# Define an embedding method that uses the model\n",
        "def encode_texts_to_embeddings(chunks: List[str]) -> List[Optional[List[float]]]:\n",
        "    try:\n",
        "        model = TextEmbeddingModel.from_pretrained(\"textembedding-gecko@002\")\n",
        "\n",
        "        # convert chunks into list[TextEmbeddingInput]\n",
        "        inputs = [TextEmbeddingInput(text=chunk, task_type=\"RETRIEVAL_DOCUMENT\") for chunk in chunks]\n",
        "        embeddings = model.get_embeddings(inputs)\n",
        "\n",
        "        # You could also generate the embeddings without the task_type.\n",
        "        # Then, you are just passing a collection of strings. In a real app\n",
        "        # test it multiple ways.\n",
        "        # The alternative would be as follows\n",
        "        # embeddings = model.get_embeddings(chunks)\n",
        "\n",
        "        return [embedding.values for embedding in embeddings]\n",
        "    except Exception:\n",
        "        return [None for _ in range(len(chunks))]\n",
        "\n",
        "\n",
        "# Generator function to yield batches of descriptions\n",
        "def generate_batches(\n",
        "    chunks: List[str], batch_size: int\n",
        ") -> Generator[List[str], None, None]:\n",
        "    for i in range(0, len(chunks), batch_size):\n",
        "        yield chunks[i : i + batch_size]\n",
        "\n",
        "\n",
        "def encode_text_to_embedding_batched(\n",
        "    chunks: List[str], api_calls_per_minute: int = 20, batch_size: int = 5\n",
        ") -> Tuple[List[bool], np.ndarray]:\n",
        "\n",
        "    embeddings_list: List[List[float]] = []\n",
        "\n",
        "    # Prepare the batches using a generator\n",
        "    batches = generate_batches(chunks, batch_size)\n",
        "\n",
        "    seconds_per_job = 60 / api_calls_per_minute\n",
        "\n",
        "    with ThreadPoolExecutor() as executor:\n",
        "        futures = []\n",
        "        for batch in tqdm(\n",
        "            batches, total=math.ceil(len(chunks) / batch_size), position=0\n",
        "        ):\n",
        "            futures.append(\n",
        "                executor.submit(functools.partial(encode_texts_to_embeddings), batch)\n",
        "            )\n",
        "            time.sleep(seconds_per_job)\n",
        "\n",
        "        for future in futures:\n",
        "            embeddings_list.extend(future.result())\n",
        "\n",
        "    is_successful = [\n",
        "        embedding is not None for sentence, embedding in zip(chunks, embeddings_list)\n",
        "    ]\n",
        "    embeddings_list_successful = np.squeeze(\n",
        "        np.stack([embedding for embedding in embeddings_list if embedding is not None])\n",
        "    )\n",
        "    return is_successful, embeddings_list_successful\n"
      ],
      "metadata": {
        "id": "m2D8TdYb3jgd"
      },
      "id": "m2D8TdYb3jgd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings_response = encode_text_to_embedding_batched(chunks, api_calls_per_minute=20)"
      ],
      "metadata": {
        "id": "-Hitlbcu4mpw"
      },
      "id": "-Hitlbcu4mpw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Need the IDs for the Vector database collection\n",
        "# Just use counters\n",
        "ids = [str(i) for i in range(len(chunks))]\n",
        "\n",
        "# chunks has the text, embeddings_response has the embeddings\n",
        "print(chunks[10][:50])\n",
        "print(embeddings_response[1][10][:10])\n",
        "\n",
        "# All the collections need the same number of items.\n",
        "print(len(chunks))\n",
        "print(len(embeddings_response[1]))\n",
        "print(len(ids))\n"
      ],
      "metadata": {
        "id": "EVjNd5BH6fWt"
      },
      "id": "EVjNd5BH6fWt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chroma DB is an open source database\n",
        "\n",
        "Google Vector Search may be a better production solution, but Chroma DB is free. So, it makes for a better demo."
      ],
      "metadata": {
        "id": "lLQg_DiDgRl9"
      },
      "id": "lLQg_DiDgRl9"
    },
    {
      "cell_type": "code",
      "source": [
        "import chromadb\n",
        "import chromadb.utils.embedding_functions as embedding_functions\n",
        "\n",
        "palm_embedding = embedding_functions.GooglePalmEmbeddingFunction(\n",
        "    api_key=API_KEY)\n",
        "\n",
        "chroma_client = chromadb.Client()\n",
        "\n",
        "# Make sure the collection does not exist\n",
        "try:\n",
        "  chroma_client.delete_collection(\"document-collection\")\n",
        "except:\n",
        "  pass\n",
        "\n",
        "# Create the collection\n",
        "chroma_collection = chroma_client.create_collection(\"document-collection\", embedding_function=palm_embedding)\n",
        "\n",
        "# Add the data to the collection\n",
        "chroma_collection.add(ids=ids, documents=chunks, embeddings=embeddings_response[1])\n",
        "chroma_collection.count()\n"
      ],
      "metadata": {
        "id": "azItU3O_CCAE"
      },
      "id": "azItU3O_CCAE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helper functions"
      ],
      "metadata": {
        "id": "PYWCROoehrxR"
      },
      "id": "PYWCROoehrxR"
    },
    {
      "cell_type": "code",
      "source": [
        "import vertexai\n",
        "from vertexai.preview.generative_models import GenerativeModel, Part\n",
        "\n",
        "# Prompt template just uses a string template to\n",
        "# format the promopt for the LLM\n",
        "prompt_template = \"\"\"\n",
        "\n",
        "Answer the users question using the following data.\n",
        "\n",
        "Data: {0}\n",
        "\n",
        "Question: {1}\n",
        "\n",
        "Answer:\n",
        "\"\"\"\n",
        "\n",
        "# Send the prompt to Gemini\n",
        "def generate(prompt):\n",
        "  model = GenerativeModel(\"gemini-pro\")\n",
        "  responses = model.generate_content(\n",
        "    prompt,\n",
        "    generation_config={\n",
        "        \"max_output_tokens\": 2048,\n",
        "        \"temperature\": 0.2,\n",
        "        \"top_p\": 1\n",
        "    },\n",
        "    )\n",
        "  return responses.text\n",
        "\n",
        "# The question is converted to an embedding.\n",
        "# Then, that embedding is used to query the vector DB.\n",
        "# the 5 closest embeddings are returned.\n",
        "def query_vector_db(question):\n",
        "\n",
        "  results = chroma_collection.query(query_texts=[question], n_results=5)\n",
        "  retrieved_documents = results['documents'][0]\n",
        "\n",
        "  # concatentate all the strings in the retrieved_documents collection\n",
        "  DATA = \" \".join(retrieved_documents)\n",
        "  return DATA\n",
        "\n",
        "def ask_question(question):\n",
        "  DATA = query_vector_db(question)\n",
        "  prompt = prompt_template.format(DATA, question)\n",
        "  answer = generate(prompt)\n",
        "  return answer\n",
        "\n"
      ],
      "metadata": {
        "id": "wXR8YfaqvIjf"
      },
      "id": "wXR8YfaqvIjf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display\n",
        "\n",
        "QUESTION1 = \"Who wrote the Wizard of Oz?\"\n",
        "QUESTION2 = \"Who were the main characters in the Wizard of Oz?\"\n",
        "QUESTION3 = \"What was the plot of the Wizard of Oz?\"\n",
        "QUESTION4 = \"Tell me about the Scarecrow?\"\n",
        "\n",
        "questions = [QUESTION1, QUESTION2, QUESTION3, QUESTION4]\n",
        "\n",
        "for question in questions:\n",
        "  answer = ask_question(question)\n",
        "  display(answer)\n",
        "  print(\"-----------------------\")\n"
      ],
      "metadata": {
        "id": "5QrnmN1LNPSc"
      },
      "id": "5QrnmN1LNPSc",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}